\documentclass[12pt, a4paper, oneside]{article}
\usepackage{amsmath, amsthm, amssymb, bm, graphicx, hyperref, mathrsfs, listings, xcolor}

\title{{\Huge{\textbf{Notes on MIT Introduction to Deep Learning}}}}
\author{Tianzong Cheng}
\date{\today}
\linespread{1.5}

\definecolor{code_background_color}{rgb}{0.95,0.95,0.92}
\lstset{language=Python, basicstyle=\ttfamily\small, backgroundcolor=\color{code_background_color}}

\begin{document}

\maketitle
\thispagestyle{empty}

\newpage
\thispagestyle{empty}
\begin{center}
    \Huge\textbf{Preface}
\end{center}
Thanks to Professor Ban for introducing this lecture to me.
\begin{flushright}
    \begin{tabular}{c}
        Tianzong Cheng \\
        \today
    \end{tabular}
\end{flushright}

\newpage
\pagenumbering{Roman}
\setcounter{page}{1}
\tableofcontents

\newpage
\setcounter{page}{1}
\pagenumbering{arabic}

\section{Lecture 1}

\subsection{Perceptron}

\begin{equation}
    \hat{y}=g(w_{0}+\sum_{i=1}^{m}x_{i}w_{i})
\end{equation}

$x_{i}$ is the input, $w_{i}$ is the weight of each input, $w_{0}$ is the bias term, and $g$ is a non-linear activation funcion.

Or we can rewrite the equation in linear algebra language.

\begin{equation}
    \hat{y} = g(w_{0}+X^{T}W)
\end{equation}

where: $X=\begin{bmatrix}
        x_{1}  \\
        \vdots \\
        x_{m}
    \end{bmatrix}$ and $W=\begin{bmatrix}
        w_{1}  \\
        \vdots \\
        w_{m}
    \end{bmatrix}$

An example of the activation function: sigmoid function. It can be intepreted as a continous version of the threshold function.

\begin{equation}
    g(z)=\sigma(z)=\frac{1}{1+e^{-z}}
\end{equation}

In TensorFlow, we can access this function by:

\begin{lstlisting}
    tf.math.sigmoid(z)
\end{lstlisting}

\subsection{Building Neural Networks with Perceptrons}

Perceptron: dot product, bias, non-linearity.

Because all inputs are densly connected to all outputs, these layers are called \textbf{Dense} layers.

The first layer, which is a hidden layer can be expressed as follows:

\begin{equation}
    z_{i}=w_{0,i}^{(1)}+\sum_{j=1}^{m}x_{j}w_{j,i}^{(1)}
\end{equation}

Then, the second layer, which calculates the final outputs, can be expressed as follows:

\begin{equation}
    \hat{y_{i}}=g(w_{0,i}^{(2)}+\sum_{j=1}^{m}g(z_{j})w_{j,i}^{(2)})
\end{equation}

We can express the whole model using two lines of codes with the help of TensorFlow:

\begin{lstlisting}
    import tensorflow as tf
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(n),
        tf.keras.layers.Dense(2)
    ])
\end{lstlisting}

By stacking these layers on top of each other, we create a sequential model.

\subsection{Loss Functions and Gradient Descent}

The \textbf{empirical loss} measures the total loss over our entire dataset. Empirical loss is also known as objective function, cost function and emprical risk.

\begin{equation}
    J(\bm{W})=\frac{1}{n}\sum_{i=1}^{n}\mathcal{L}(f(x^{(i)};\bm{W}),y^{(i)})
\end{equation}

In this equation, $J$ is a function of the weights of the neuron network. We want to find the network weights that \textbf{achieve the lowest loss}. 

First, we randomly pick an initial $(w_{0},w_{1})$. Then, by taking a small step in the opposite direction of gradient at this point, we can get closer to where the loss is the lowest. Repeat this step until convergence.

\begin{lstlisting}
    import tensorflow as tf
    weights = tf.Variable([tf.random.normal()])
    while True:
        with tf.GradientTape() as g:
            loss = compute_loss(weights)
            gradient = g.gradient(loss, weights)
        weights = weights - lr * gradient
\end{lstlisting}

\subsection{Backpropagation}

\begin{equation}
    \frac{\partial J(\bm{W})}{\partial w_{1}}=\frac{\partial J(\bm{W})}{\partial \hat{y}}\frac{\partial\hat{y}}{\partial z_{1}}\frac{\partial z_{1}}{\partial w_{1}}
\end{equation}

Repeat this for \textbf{every weight in the network} using gradients from later layers. We can see backpropagation is simply an instantiation of the chain rule.

\subsection{Bathced Gradient Descent}

Compute the gradient of a batch of (usually around a hundred) data points, rather than a single data point or all the data points. This is a fast and accurate way of estimating the gradient.

\subsection{Regularization}

The problem of overfitting describes that the model is too complex and does not generalize well.

Regularization is a technique that constrains our optimization problem to discourage complex models. It helps improving generalization of our model on unseen data.

The idea of \textbf{dropout} is randomly selecting some (typically half) activations to $0$. The idea of \textbf{early stopping} is to stop training before we have a chance to overfit.

\end{document}